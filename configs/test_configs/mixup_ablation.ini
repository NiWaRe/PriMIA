[config]
batch_size = 200
train_resolution = 224
;inference_resolution = 512
test_batch_size = 200
test_interval = 1
validation_split = 10
epochs = 20
lr = 1e-4
end_lr = 1e-5
restarts = 0
beta1 = 0.5
beta2 = 0.99
weight_decay = 5e-4
;momentum = 0.5
seed = 1
log_interval = 1
optimizer = Adam
model = resnet-18
pretrained = yes
weight_classes = no
pooling_type = max

[augmentation]
vertical_flip_prob = 0
rotation = 0
;translate = 0.0
scale = 0
shear = 0
noise_std = 0
noise_prob = 0
mixup = yes
mixup_lambda = 0.75
mixup_prob = 1.

[federated]
sync_every_n_batch = 2
wait_interval = 0.1
keep_optim_dict = no
repetitions_dataset = 1
weighted_averaging = yes
