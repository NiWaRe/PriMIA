[config]
batch_size = 32
train_resolution = 28
;inference_resolution = 512
test_batch_size = 128
test_interval = 1
validation_split = 10
epochs = 20
lr = 1e-3
end_lr = 1e-4
restarts = 1
beta1 = 0.5
beta2 = 0.99
weight_decay = 5e-4
;momentum = 0.5
deterministic = yes
seed = 1
log_interval = 10
optimizer = Adam
model = simpleconv
pretrained = no
weight_classes = no
pooling_type = max


[augmentation]
rotation = 30
translate = 0.0
scale = 0.15
shear = 10
mixup = no
mixup_prob = 0.0

[albumentations]
noise_std = 0.05
noise_prob = 0.5
clahe = no
overall_prob = 0.0
individual_probs = 0.0
randomgamma = no
randombrightness = no
blur = no
elastic = no   
optical_distortion = no
grid_distortion = no
grid_shuffle = no
hsv = no
invert = no
cutout = no
shadow = no
fog = no
sun_flare = no
solarize = no
equalize = no
grid_dropout = no

[federated]
sync_every_n_batch = 10
wait_interval = 0.1
keep_optim_dict = no
weighted_averaging = yes
repetitions_dataset=1
